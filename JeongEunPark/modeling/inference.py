import torch
import torch.nn.functional as F
from transformers import PreTrainedTokenizerFast
from model import EncoderCNN, DecoderRNN
from preprocess import normalize_caption
from PIL import Image
from torchvision.transforms import Compose, Resize, ToTensor, Normalize
import csv
from tqdm import tqdm 
import pandas as pd 
import torch
import torch.nn.functional as F

def generate_caption(image, encoder, decoder, tokenizer, device, max_length=82, beam_width=3, use_beam_search=False):
    encoder.eval()
    decoder.eval()

    with torch.no_grad():
        features = encoder(image)  # [1, embed_size]

        if not use_beam_search:
            # ğŸ”¹ Greedy decoding
            generated = [tokenizer.bos_token_id]

            for _ in range(max_length):
                input_ids = torch.tensor([generated], device=device)  # [1, T]
                attention_mask = torch.ones_like(input_ids, device=device)  # [1, T]

                outputs = decoder(
                    features=features,
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                logits = outputs.logits[:, -1, :]
                next_token = torch.argmax(logits, dim=-1).item()
                generated.append(next_token)

                if next_token == tokenizer.eos_token_id:
                    break

            return tokenizer.decode(generated, skip_special_tokens=True)

        else:
            # ğŸ”¹ Beam Search
            beams = [([tokenizer.bos_token_id], 0.0)]

            for _ in range(max_length):
                new_beams = []

                for seq, score in beams:
                    input_ids = torch.tensor([seq], device=device)
                    attention_mask = torch.ones_like(input_ids, device=device)

                    outputs = decoder(
                        features=features,
                        input_ids=input_ids,
                        attention_mask=attention_mask
                    )

                    logits = outputs.logits[:, -1, :]
                    log_probs = F.log_softmax(logits, dim=-1).squeeze(0)

                    topk_log_probs, topk_ids = torch.topk(log_probs, beam_width)

                    for log_prob, idx in zip(topk_log_probs, topk_ids):
                        new_seq = seq + [idx.item()]
                        new_score = score + log_prob.item()
                        new_beams.append((new_seq, new_score))

                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]

                if any(seq[-1] == tokenizer.eos_token_id for seq, _ in beams):
                    break

            best_seq = beams[0][0]
            return tokenizer.decode(best_seq, skip_special_tokens=True)
        
def generate_captions_for_valset(val_dataloader, encoder, decoder, tokenizer, device, num_samples=5):
    encoder.eval()
    decoder.eval()

    results = []
    count = 0

    for batch in val_dataloader:
        if count >= num_samples:
            break

        images, input_ids, _, img_paths = batch  # â† img_path ë°›ê¸°
        images = images.to(device)
        input_ids = input_ids.to(device)

        with torch.no_grad():
            pred_caption = generate_caption(
                image=images,
                encoder=encoder,
                decoder=decoder,
                tokenizer=tokenizer,
                device=device,
                use_beam_search=True
            )

        ref_caption = tokenizer.decode(input_ids[0], skip_special_tokens=True)
        results.append((ref_caption, pred_caption, img_paths[0]))  # img_path í¬í•¨
        count += 1

    return results
# def generate_caption(image_path, encoder, decoder, tokenizer, device, max_length=82, beam_width=3, use_beam_search=False):
#     transform = Compose([
#         Resize((224, 224), antialias=True),
#         ToTensor(),
#         Normalize(mean=[0.485, 0.456, 0.406], 
#                   std=[0.229, 0.224, 0.225])
#     ])
#     image = Image.open(image_path).convert('RGB')
#     image = transform(image).unsqueeze(0).to(device)

#     encoder.eval() 
#     decoder.eval()
#     # ì´ë¯¸ì§€ ì„ë² ë”©
#     with torch.no_grad():
#         features = encoder(image)  # [1, embed_size]
#         if not use_beam_search:
#             # ê¸°ì¡´ Greedy decoding
#             generated = [tokenizer.bos_token_id]
#             for _ in range(max_length):
#                 input_ids = torch.tensor([generated]).to(device)
#                 attention_mask = torch.ones_like(input_ids).to(device)

#                 outputs = decoder(features, input_ids, attention_mask=attention_mask)
#                 next_token_logits = outputs.logits[:, -1, :]
#                 next_token = torch.argmax(next_token_logits, dim=-1).item()

#                 generated.append(next_token)
#                 if next_token == tokenizer.eos_token_id:
#                     break

#             caption = tokenizer.decode(generated, skip_special_tokens=True)
#             return caption

#         else:
#             # Beam Search
#             beams = [( [tokenizer.bos_token_id], 0 )]  # (í† í° ì‹œí€€ìŠ¤, ëˆ„ì  ë¡œê·¸í™•ë¥ )

#             for _ in range(max_length):
#                 new_beams = []
#                 for seq, score in beams:
#                     input_ids = torch.tensor([seq]).to(device)
#                     attention_mask = torch.ones_like(input_ids).to(device)

#                     outputs = decoder(features, input_ids, attention_mask=attention_mask)
#                     logits = outputs.logits[:, -1, :]  # [1, vocab_size]
#                     log_probs = F.log_softmax(logits, dim=-1).squeeze(0)  # [vocab_size]

#                     topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)

#                     for log_prob, idx in zip(topk_log_probs, topk_indices):
#                         next_seq = seq + [idx.item()]
#                         next_score = score + log_prob.item()
#                         new_beams.append((next_seq, next_score))

#                 # top-k ë¹” ì„ íƒ
#                 beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]

#                 # EOSë¡œ ëë‚˜ëŠ” ì‹œí€€ìŠ¤ê°€ ìˆìœ¼ë©´ ì¢…ë£Œ
#                 if any(seq[-1] == tokenizer.eos_token_id for seq, _ in beams):
#                     break

#             # ê°€ì¥ í™•ë¥  ë†’ì€ ì‹œí€€ìŠ¤ ì„ íƒ
#             best_seq = beams[0][0]
#             caption = tokenizer.decode(best_seq, skip_special_tokens=True)
#             return caption


# def main():
#     DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     EMBED_SIZE = 256
#     TOKENIZER_NAME = "skt/kogpt2-base-v2"

#     ENCODER_PATH = "state_dict/encoder_best_ep70.pt"
#     DECODER_PATH = "state_dict/decoder_best.pt"
#     CSV_PATH = "test_data.csv"
#     DEBUG_SAMPLES = 5

#     tokenizer = PreTrainedTokenizerFast.from_pretrained(
#         TOKENIZER_NAME,
#         bos_token='</s>',
#         eos_token='</s>',
#         unk_token='<unk>',
#         pad_token='<pad>',
#         mask_token='<mask>'
#     )
#     encoder = EncoderCNN(EMBED_SIZE).to(DEVICE)
#     decoder = DecoderRNN(EMBED_SIZE, model_name=TOKENIZER_NAME).to(DEVICE)
#     decoder.kogpt2.resize_token_embeddings(len(tokenizer))

#     encoder.load_state_dict(torch.load(ENCODER_PATH, map_location=DEVICE))
#     decoder.load_state_dict(torch.load(DECODER_PATH, map_location=DEVICE))

#     encoder.eval()
#     decoder.eval()

#     # test_data.csv ë¶ˆëŸ¬ì˜¤ê¸°
#     df = pd.read_csv(CSV_PATH)

#     # ì¶”ë¡  ê²°ê³¼ ì €ì¥ìš©
#     results = []
#     MAX_SAMPLES = 50  # ì¶”ë¡ í•  ìƒ˜í”Œ ìˆ˜
#     print(f"\n[âœ“] ì´ {len(df)}ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì¶”ë¡  ì‹œì‘...\n")

#     for i, row in tqdm(df.iterrows(), total=MAX_SAMPLES):
#         img_path = row["img_path"]
#         ref_caption = row["caption"]

#         pred_caption = generate_caption(
#             image_path=img_path,
#             encoder=encoder,
#             decoder=decoder,
#             tokenizer=tokenizer,
#             device=DEVICE,
#             max_length=82,
#             beam_width=5,
#             use_beam_search=True
#         )

#         results.append({
#             "img_path": img_path,
#             "reference": ref_caption,
#             "generated": pred_caption
#         })

#     # ê²°ê³¼ ì €ì¥
#     output_df = pd.DataFrame(results)
#     output_df.to_csv("inference_results_beamsearch.csv", index=False, encoding="utf-8-sig")
#     print("\n[âœ“] inference_results_beamsearch.csv ì €ì¥ ì™„ë£Œ!")

# if __name__ == "__main__":
#     main()